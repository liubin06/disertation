
%%% Local Variables:
%%% mode: Xelatex
%%% TeX-master: t
%%% End:

\ctitle{基于对比学习的推荐算法研究}

\xuehao{D201881076} \schoolcode{10487}
\csubjectname{信息与通信工程} \cauthorname{刘斌}
\csupervisorname{王邦} \csupervisortitle{教授}
\defencedate{2023~年~11~月~27~日} \grantdate{}
\chair{刘文予}%
\firstreviewer{A} \secondreviewer{A} \thirdreviewer{B}

\etitle{Contrastive Learning for Personalized Recommendation}
\edegree{Doctor of Engineering}
\esubject{Information and Communication Engineering}
\eauthor{LIU Bin}
\esupervisor{WANG Bang }
%定义中英文摘要和关键字
\cabstract{
在互联网飞速发展的信息时代，推荐系统在电商、社交媒体、新闻阅读等领域发挥着重要的作用。自监督推荐对输入数据进行变换或处理提取自监督信号，有助于缓解数据稀疏、虚假相关和对抗攻击等问题，受到了广泛的关注。基于对比学习的推荐算法是自监督推荐的主要实现方式，尽管在自监督设置下表现了出色的泛化能力，但与有监督设置下的对比型推荐算法存在较大差距。如何从不完整标注的数据中学到良好的特征表示，是机器学习基础理论的需要，也是提升推荐算法准确性的现实需求。

本文聚焦自监督推荐场景，针对对比型推荐算法的伪正例、伪负例和优化目标偏离问题，提出了相应的正例去噪算法、负例采样算法和损失函数校正算法。本文的创新和贡献如下：

针对伪正例问题，提出了自适应正例去噪的推荐算法。将从含噪成对比较的数据中学习排序的问题形式化为一个含有隐变量的最大后验估计问题。所提出的算法采用期望最大化框架：在期望步骤中使用贝叶斯推断来估计交互的置信度指标；在最大化步骤中固定置信度指标，更新参数学习用户和物品的特征表示。在合成的噪声数据集上表现出了更好的鲁棒性，在真实的数据集上top-5准确率相对于最优的对比算法平均提升6.2\%。

针对伪负例问题，提出了基于最优负采样准则的推荐算法。提出了后验概率意义上未标注样本是真负例的负信号测度，结合了静态的先验信息和动态的样本信息，较好地综合了现有的两种提取负信号的范式；进而提出了最优负采样准则，是经验采样风险最小化意义上的最优采样准则。该算法在采样误差率和采样样本信息量优于同类方法，相对于最优的对比算法top-5准确率平均提升3.4\%。

针对自监督设置下成对损失优化目标偏离问题，提出了基于去偏成对损失的推荐算法。通过校正伪负样本导致的概率估计偏差，修正自监督设置下成对损失的梯度，得到了去偏成对损失。该损失是有监督设置下的成对损失的一致估计，在保持严格的相对于成对学习的线性时间复杂度情况下，top-5准确率相对于最优的对比算法平均提升3.2\%。

针对自监督设置下对比损失优化目标偏离问题，提出了基于贝叶斯自监督对比损失的推荐算法。通过重要性权重来纠正从未标注数据中随机选择的负样本引入的偏差，得到了贝叶斯自监督对比损失。对于单个权重，其正比于未标注样本是真负例的后验概率估计；对于损失函数值，该损失是有监督设置下对比损失的一致估计。贝叶斯自监督对比损失可以灵活统一地执行硬负例挖掘和伪负例去偏任务，top-5准确率相对于最优的对比算法平均提升2.2\%。此外在数值仿真实验、无监督图像分类等多个任务中验证了贝叶斯自监督对比损失的有效性。

综上，本文针对自监督设置下的对比型推荐算法的三个关键组件：正例、负例和损失函数做出了改进，为后续推荐算法和对比学习研究提供了有效的支撑。
}
\ckeywords{推荐算法；对比学习；成对学习；自监督学习；负采样}

\eabstract{In the rapidly evolving information age of the Internet, recommendation systems play a crucial role in various domains such as e-commerce, social media, and news reading. Self-supervised recommendation, which leverages transformations and processing of implicit feedback data to extract self-supervisory signals, has gained significant attention due to its potential to address issues like data sparsity, false correlations, and adversarial attacks. Contrastive learning-based recommendation algorithms are the primary implementation approach for self-supervised recommendation. While they demonstrate excellent generalization capability in self-supervised settings, a significant gap exists compared to contrastive recommendation algorithms under supervised settings. Learning better feature representations from incompletely annotated data is not only a theoretical requirement for machine learning but also a practical need to enhance the accuracy of recommendation algorithms.
	
This dissertation focuses on the self-supervised recommendation scenario and addresses the issues of false positive samples, false negative samples, and optimization objective deviation in contrastive recommendation algorithms. We propose corresponding algorithms for positive example denoising, negative example sampling, and loss function correction. The innovations and contributions of this dissertation are as follows:
	
For the problem of false positive samples, we propose an adaptive denoising recommendation algorithm for learning personalized ranking from noisy implicit feedback data. We formalize the problem of learning ranking from noisy pairwise comparison data as a maximum a posteriori estimation problem with latent variables. The proposed algorithm adopts an Expectation-Maximization framework: in the Expectation step, Bayesian inference is used to estimate confidence indicators for interactions, and in the Maximization step, the confidence indicators are fixed to update the feature representations of users and items. The algorithm demonstrates better robustness on synthetic noisy datasets and achieves an average improvement of 6.2\% in top-5 precision compared to the optimal baseline algorithm on real datasets.
	
For the problem of false negative samples, we propose a recommendation algorithm based on the optimal negative sampling criterion. We introduce a negative signal measurement in terms of posterior probability, combining static prior information and dynamic sample information, effectively integrating the two main paradigms for extracting negative signals. This approach provides a flexible interface for modeling prior probabilities using auxiliary information. Furthermore, we propose the optimal negative sampling criterion, which is the theoretically optimal sampling rule in terms of empirical sampling risk minimization. The algorithm outperforms similar methods in terms of sampling error rate and sampling sample information. It achieves an average improvement of 3.4\% in top-5 precision compared to the optimal baseline algorithm.
	
For the problem of optimization objective deviation of pairwise loss, we propose a recommendation algorithm based on debiased pairwise loss. By correcting probability estimation biases caused by false negative samples, we approximate the gradient to approximate the gradient of pairwise loss under supervised settings, resulting in a debiased pairwise loss. This loss provides a consistent estimation of the pairwise loss under supervised settings without the need for side information for supervision. Based on the debiased pairwise loss, The algorithm achieves an average improvement of 3.2\% in top-5 precision compared to the optimal baseline algorithm, while maintaining linear time complexity relative to BPR.
	
For the problem of optimization objective deviation of contrastive loss, we propose a recommendation algorithm based on Bayesian self-supervised contrastive loss. By using importance weights to correct biases introduced by randomly selected negative samples from unlabeled data, we obtain the Bayesian self-supervised contrastive loss. For the individual weight, it is proportional to the posterior probability estimate that the unlabeled sample being true negative, allowing flexible execution of hard negative mining and false negative sample debiasing tasks. From a numerical perspective, this loss provides a consistent estimation of the supervised contrastive loss, thereby improving the generalization performance of recommendation models. It achieves an average improvement of 2.2\% in top-5 precision compared to the optimal baseline algorithm. Furthermore, the effectiveness of the Bayesian self-supervised contrastive loss is further verified in numerical experiments and  un-supervised image classification task.
	
In conclusion, this dissertation proposes improvements for three key components of contrastive recommendation algorithms in self-supervised settings: positive examples, negative examples, and loss functions. These improvements provide effective support for future recommendation algorithms and contrastive learning research. }

\ekeywords{Recommender System, Contrastive Learning, Pairwise Learning, Self-supervised Learning, Negative Sampling}
